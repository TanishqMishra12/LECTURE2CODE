# ── LLM Backend ──────────────────────────────────────────────────────────────
# Which LLM backend to use: "ollama" or "openai"
LLM_BACKEND=ollama

# ── Ollama (required when LLM_BACKEND=ollama) ────────────────────────────────
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama:13b

# ── OpenAI (required when LLM_BACKEND=openai) ────────────────────────────────
# OPENAI_API_KEY=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
OPENAI_MODEL=gpt-4o

# ── Transcript handling ───────────────────────────────────────────────────────
MAX_TRANSCRIPT_TOKENS=6000
CACHE_TRANSCRIPTS=true

# ── Rate limiting ─────────────────────────────────────────────────────────────
RATE_LIMIT_PER_HOUR=10

# ── Session management ────────────────────────────────────────────────────────
SESSION_TTL_SECONDS=3600

# ── CORS ──────────────────────────────────────────────────────────────────────
CORS_ORIGINS=http://localhost:5173

# ── Logging ───────────────────────────────────────────────────────────────────
LOG_LEVEL=info

# ── Streaming ─────────────────────────────────────────────────────────────────
ENABLE_STREAMING=true
