# ── LLM Backend ──────────────────────────────────────────────────────────────
# Which LLM backend to use: "ollama" or "openai"
LLM_BACKEND=ollama

# ── Ollama (required when LLM_BACKEND=ollama) ────────────────────────────────
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama:13b

# ── OpenAI (required when LLM_BACKEND=openai) ────────────────────────────────
# OPENAI_API_KEY=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
OPENAI_MODEL=gpt-4o

# ── Transcript handling ───────────────────────────────────────────────────────
MAX_TRANSCRIPT_TOKENS=6000
CACHE_TRANSCRIPTS=true

# ── Rate limiting ─────────────────────────────────────────────────────────────
RATE_LIMIT_PER_HOUR=10

# ── Session management ────────────────────────────────────────────────────────
SESSION_TTL_SECONDS=3600

# ── CORS ──────────────────────────────────────────────────────────────────────
CORS_ORIGINS=http://localhost:5173

# ── Logging ───────────────────────────────────────────────────────────────────
LOG_LEVEL=info

# ── Streaming ─────────────────────────────────────────────────────────────────
ENABLE_STREAMING=true
